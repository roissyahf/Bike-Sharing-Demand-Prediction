{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3948,"databundleVersionId":32624,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predict Bike Sharing Demand with AutoGluon\n\n**Roissyah Fernanda Khoiroh**","metadata":{}},{"cell_type":"markdown","source":"## Project: Predict Bike Sharing Demand with AutoGluon\nThis notebook is a template with each step that you need to complete for the project.\n\nPlease fill in your code where there are explicit `?` markers in the notebook. You are welcome to add more cells and code as you see fit.\n\nOnce you have completed all the code implementations, please export your notebook as a HTML file so the reviews can view your code. Make sure you have all outputs correctly outputted.\n\n`File-> Export Notebook As... -> Export Notebook as HTML`\n\nThere is a writeup to complete as well after all code implememtation is done. Please answer all questions and attach the necessary tables and charts. You can complete the writeup in either markdown or PDF.\n\nCompleting the code template and writeup template will cover all of the rubric points for this project.\n\nThe rubric contains \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. The stand out suggestions are optional. If you decide to pursue the \"stand out suggestions\", you can include the code in this notebook and also discuss the results in the writeup file.","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Create an account with Kaggle and download API key","metadata":{}},{"cell_type":"markdown","source":"## Step 2: Download the Kaggle dataset using the kaggle python library","metadata":{}},{"cell_type":"markdown","source":"### Open up Sagemaker Studio and use starter template","metadata":{}},{"cell_type":"markdown","source":"1. Notebook should be using a `ml.t3.medium` instance (2 vCPU + 4 GiB)\n2. Notebook should be using kernal: `Python 3 (MXNet 1.8 Python 3.7 CPU Optimized)`","metadata":{}},{"cell_type":"markdown","source":"### Install packages","metadata":{}},{"cell_type":"code","source":"!pip install -U pip -q\n!pip install -U setuptools wheel -q\n!pip install -U \"mxnet<2.0.0\" bokeh==2.0.1 -q\n!pip install autogluon --no-cache-dir -q\n# Without --no-cache-dir, smaller aws instances may have trouble installing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setup Kaggle API Key","metadata":{}},{"cell_type":"code","source":"# create the .kaggle directory and an empty kaggle.json file\n!mkdir -p /root/.kaggle\n!touch /root/.kaggle/kaggle.json\n!chmod 600 /root/.kaggle/kaggle.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill in your user name and key from creating the kaggle account and API token file\nimport json\nkaggle_username = \"\"\nkaggle_key = \"\"\n\n# Save API token the kaggle.json file\nwith open(\"/root/.kaggle/kaggle.json\", \"w\") as f:\n    f.write(json.dumps({\"username\": kaggle_username, \"key\": kaggle_key}))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Download and explore dataset","metadata":{}},{"cell_type":"code","source":"# Download the dataset, it will be in a .zip file so you'll need to unzip it as well.\n!kaggle competitions download -c bike-sharing-demand\n# If you already downloaded it you can use the -o command to overwrite the file\n!unzip -o bike-sharing-demand.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom autogluon.tabular import TabularPredictor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the train dataset in pandas by reading the csv\n# Set the parsing of the datetime column so you can use some of the `dt` features in pandas later\ntrain = pd.read_csv('train.csv')\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Simple output of the train dataset to view some of the min/max/varition of the dataset features.\ntrain.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the test pandas dataframe in pandas by reading the csv, remember to parse the datetime!\ntest = pd.read_csv('test.csv')\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Same thing as train and test dataset\nsubmission = pd.read_csv('sampleSubmission.csv')\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: Train a model using AutoGluonâ€™s Tabular Prediction","metadata":{}},{"cell_type":"markdown","source":"Requirements:\n* We are prediting `count`, so it is the label we are setting.\n* Ignore `casual` and `registered` columns as they are also not present in the test dataset. \n* Use the `root_mean_squared_error` as the metric to use for evaluation.\n* Set a time limit of 10 minutes (600 seconds).\n* Use the preset `best_quality` to focus on creating the best model.","metadata":{}},{"cell_type":"code","source":"df_train_ori = train.drop(columns=['datetime', 'casual', 'registered'], axis=1)\n\npredictor = TabularPredictor(label=\"count\").fit(\n        train_data = df_train_ori,\n        time_limit = 600,\n        presets= \"best_quality\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Review AutoGluon's training run with ranking of models that did the best.","metadata":{}},{"cell_type":"code","source":"predictor.fit_summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor.leaderboard(silent=True).plot(kind=\"bar\", x=\"model\", y=\"score_val\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_1_eval = predictor.evaluate(df_train_ori)\ntrain_1_eval","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create predictions from test dataset","metadata":{}},{"cell_type":"code","source":"# set up df_test\ndf_test_ori = test.drop(columns=['datetime'])\n\n# prediction on test set\ncount_predictions = predictor.predict(df_test_ori)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Describe the `predictions` series to see if there are any negative values\ncount_predictions.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### NOTE: Kaggle will reject the submission if we don't set everything to be > 0.","metadata":{}},{"cell_type":"code","source":"# How many negative values do we have?\nprint((count_predictions < 0).sum())\n\n# Set them to zero\ncount_predictions[count_predictions < 0] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Set predictions to submission dataframe, save, and submit","metadata":{}},{"cell_type":"code","source":"submission[\"datetime\"]= test[\"datetime\"]\nsubmission[\"count\"] = count_predictions\nsubmission.to_csv(\"submission_1.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!kaggle competitions submit -c bike-sharing-demand -f submission_1.csv -m \"first raw submission\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### View submission via the command line or in the web browser under the competition's page - `My Submissions`","metadata":{}},{"cell_type":"code","source":"!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Initial score of `1.32509`","metadata":{}},{"cell_type":"markdown","source":"## Step 4: Exploratory Data Analysis and Creating an additional feature\n* Any additional feature will do, but a great suggestion would be to separate out the datetime into hour, day, or month parts.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime as dt\nimport numpy as np\n\nimport warnings\ndef fxn():\n    warnings.warn(\"deprecated\", DeprecationWarning)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fxn()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select numerical columns\nnumerical_cols = train.select_dtypes(include=['float64', 'int64', 'int32'])\n\n# Determine number of rows and columns for subplots\nnum_cols = numerical_cols.shape[1]\nnum_rows = (num_cols - 1) // 2 + 1  # Round up\n\n# Create subplots\nfig, axes = plt.subplots(num_rows, 2, figsize=(20, num_rows * 5))\n\n# Flatten axes if necessary\naxes = axes.flatten()\n\n# Plot histograms\nfor i, col in enumerate(numerical_cols.columns):\n    sns.histplot(numerical_cols[col], ax=axes[i])\n    axes[i].set_title(col)\n\n# Remove empty subplots\nfor j in range(i+1, len(axes)):\n    fig.delaxes(axes[j])\n\n# Adjust layout\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert object to timestamp\ntrain[\"datetime\"] = pd.to_datetime(train[\"datetime\"])\ntest[\"datetime\"] = pd.to_datetime(test[\"datetime\"])\n\n# create a new feature\ntrain[\"hour\"] = train[\"datetime\"].dt.hour\n#train[\"weekend\"]\n#train[\"weekday\"]\ntrain[\"day\"] = train[\"datetime\"].dt.day\ntrain[\"month\"] = train[\"datetime\"].dt.month\ntrain[\"year\"] = train[\"datetime\"].dt.year\n\ntest[\"hour\"] = test[\"datetime\"].dt.hour\n#[\"weekend\"]\n#[\"weekday\"]\ntest[\"day\"] = test[\"datetime\"].dt.day\ntest[\"month\"] = test[\"datetime\"].dt.month\ntest[\"year\"] = test[\"datetime\"].dt.year","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make category types for these so models know they are not just numbers\n* AutoGluon originally sees these as ints, but in reality they are int representations of a category.\n* Setting the dtype to category will classify these as categories in AutoGluon.","metadata":{}},{"cell_type":"code","source":"# int64 to category\ntrain[\"season\"] = train[\"season\"].astype('category')\ntrain[\"weather\"] = train[\"weather\"].astype('category')\ntest[\"season\"] = test[\"season\"].astype('category')\ntest[\"weather\"] = test[\"weather\"].astype('category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View are new feature\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# histplot with new features\n# Select numerical columns\nnumerical_cols_2 = train.select_dtypes(include=['float64', 'int64', 'int32'])\n\n# Determine number of rows and columns for subplots\nnum_cols = numerical_cols_2.shape[1]\nnum_rows = (num_cols - 1) // 2 + 1  # Round up\n\n# Create subplots\nfig, axes = plt.subplots(num_rows, 2, figsize=(20, num_rows * 5))\n\n# Flatten axes if necessary\naxes = axes.flatten()\n\n# Plot histograms\nfor i, col in enumerate(numerical_cols_2.columns):\n    sns.histplot(numerical_cols_2[col], ax=axes[i])\n    axes[i].set_title(col)\n\n# Remove empty subplots\nfor j in range(i+1, len(axes)):\n    fig.delaxes(axes[j])\n\n# Adjust layout\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correlation metric\ncorr = train.corr()\nplt.figure(figsize=(16,8))\n\n#masking the upper triangle part (since matrix is repetitive)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap=\"Blues\", annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# time series analysis\n\ndef plot_line_graph(data, x_col, y_col, title, figsize=(25, 7), facecolor=\"#fff\", grid_color=\"lightgrey\", line_color=\"orangered\", label_fontsize=15, tick_fontsize=12):\n    fig = plt.figure(figsize=figsize)\n    fig.set_facecolor(facecolor)\n    ax = fig.add_subplot()\n    ax.set_facecolor(facecolor)\n    ax.grid(color=grid_color, alpha=0.7, linewidth=1, axis=\"both\", zorder=0)\n    sns.lineplot(x=x_col, y=y_col, color=line_color, err_style=None, data=data, linewidth=4, ax=ax, zorder=2)\n    ax.yaxis.set_tick_params(color=\"#000\", labelsize=tick_fontsize, pad=5, length=0)\n    ax.set_ylabel(y_col.capitalize(), fontsize=label_fontsize, fontfamily=\"serif\", labelpad=10)\n    ax.set_xlabel(x_col.capitalize(), fontsize=label_fontsize, fontfamily=\"serif\", labelpad=10)\n    ax.xaxis.set_tick_params(color=\"#000\", labelsize=tick_fontsize, pad=5, length=0)\n    ax.yaxis.set_tick_params(color=\"#000\", labelsize=tick_fontsize, pad=5, length=0)\n    ax.set_title(title, loc=\"left\", color=\"#000\", fontsize=label_fontsize * 1.5, pad=5, fontweight=\"bold\", fontfamily=\"serif\", y=1.05, zorder=3)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# monthly bike demand count\nplot_line_graph(train, \"month\", \"count\", \"Monthly bike demand count\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# daily bike demand count\nplot_line_graph(train, \"day\", \"count\", \"Daily bike demand count\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 5: Rerun the model with the same settings as before, just with more features","metadata":{}},{"cell_type":"code","source":"df_train_added = train.drop(columns=['datetime', 'casual', 'registered'], axis=1)\n\npredictor_new_features = TabularPredictor(label=\"count\").fit(\n    train_data = df_train_added,\n    time_limit = 600,\n    presets= \"best_quality\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor_new_features.fit_summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor_new_features.leaderboard(silent=True).plot(kind=\"bar\", x=\"model\", y=\"score_val\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor_new_features.evaluate(df_train_added)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_added = test.drop(columns=\"datetime\", axis=1)\n\n# predictions\ncount_predictions_new_feat = predictor_new_features.predict(df_test_added)\n\n# print count of negative prediction\nprint((count_predictions_new_feat< 0).sum())\n\n# Remember to set all negative values to zero\ncount_predictions_new_feat[count_predictions_new_feat < 0] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Same submitting predictions\nmy_datetime = test[\"datetime\"]\ncount_predictions_new_feat\n\nsubmission_new_features = pd.DataFrame(\n    list(zip(my_datetime, count_predictions_new_feat)),\n    columns = [\"datetime\", \"count\"]\n)\n\nsubmission_new_features\nsubmission_new_features.to_csv(\"submission_new_features.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!kaggle competitions submit -c bike-sharing-demand -f submission_new_features.csv -m \"new features\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### New Score of `0.48189`","metadata":{}},{"cell_type":"markdown","source":"## Step 6: Hyper parameter optimization I\n* There are many options for hyper parameter optimization.\n* Options are to change the AutoGluon higher level parameters or the individual model hyperparameters.\n* The hyperparameters of the models themselves that are in AutoGluon. Those need the `hyperparameter` and `hyperparameter_tune_kwargs` arguments.","metadata":{}},{"cell_type":"code","source":"import autogluon.core as ag\nfrom ray import tune","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hyperpar_tune = {\n    'searcher': 'random',  # Use random search for hyperparameter tuning, option: \"random\"\n    'max_tune_time': 3600,  # Maximum time to spend on hyperparameter tuning (in seconds)\n    \"scheduler\": \"local\",\n    \"num_trials\": 4\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hyperparameter for FastAI Neural Network\nfastai_opt = {\n    'layer': tune.grid_search([200,100]),\n    'lr': tune.grid_search([0.0001, 0.001]),\n    'ps': tune.grid_search([0.0001, 0.002]),\n    'emb_drop': tune.grid_search([0.0001, 0.005]),\n    'epochs': tune.grid_search([8,24]),\n    'early.stopping.patience': 5\n}\n\n# hyperparameter for CatBoost\ncatboost_opt = {\n    'depth': tune.grid_search([5,25]),\n    'l2_leaf_reg': tune.grid_search([0.05, 5])\n}\n\nhypo_1 = {\n    'CAT': catboost_opt,\n    'FASTAI': fastai_opt\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor_new_hpo = TabularPredictor(path='AutogluonModels/ag-hypo-1', \n                                     label=\"count\").fit(\n    train_data = df_train_added,\n    time_limit = 600,\n    presets= \"best_quality\",\n    hyperparameters = hypo_1,\n    hyperparameter_tune_kwargs = hyperpar_tune\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor_new_hpo.fit_summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor_new_hpo.leaderboard(silent=True).plot(kind=\"bar\", x=\"model\", y=\"score_val\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor_new_hpo.evaluate(df_train_added)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction\ncount_with_hpo_1 = predictor_new_hpo.predict(df_test_added)\n\n# Remember to set all negative values to zero\ncount_with_hpo_1[count_with_hpo_1 < 0] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Same submitting predictions\nmy_datetime = test[\"datetime\"]\ncount_with_hpo_1\n\nsubmission_hpo_1 = pd.DataFrame(\n    list(zip(my_datetime, count_with_hpo_1)),\n    columns = [\"datetime\", \"count\"]\n)\n\nsubmission_hpo_1.to_csv(\"submission_hpo_1.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!kaggle competitions submit -c bike-sharing-demand -f submission_hpo_1.csv -m \"new features with hyperparameters: NNFastAI, CAT\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### New Score of `1.19842`","metadata":{}},{"cell_type":"markdown","source":"## Step 7: Hyperparameter Tuning II","metadata":{}},{"cell_type":"code","source":"hyperpar_tune_2 = {\n    'searcher': 'random',  # Use random search for hyperparameter tuning, option: \"random\"\n    'max_tune_time': 3600,  # Maximum time to spend on hyperparameter tuning (in seconds)\n    \"scheduler\": \"local\",\n    \"num_trials\": 4\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define search space \ngbm_opt = {\n    'learning_rate': tune.loguniform(0.01, 0.5),  # Adjust the learning rate\n    'max_depth': tune.randint(3, 10),            # Maximum depth of the trees\n    'min_child_weight': tune.randint(1, 10),      # Minimum sum of instance weight (hessian) needed in a child\n    }\n\nrf_opt = {\n    'ag_args': {'name_suffix': 'MSE', 'problem_types':['regression']},\n    'max_depth': tune.randint(3, 20),             # Maximum depth of the trees\n    'min_samples_split': tune.randint(2, 20),      # Minimum number of samples required to split a node\n    'min_samples_leaf': tune.randint(1, 20),       # Minimum number of samples required at each leaf node\n    }\n\ncatb_opt = {\n    #'iterations': tune.randint(50, 200),\n    'learning_rate': tune.loguniform(0.01, 0.5),   # Learning rate\n    'depth': tune.randint(3, 10),                  # Depth of the trees\n    'l2_leaf_reg': tune.loguniform(1e-5, 100) \n    }\n\nxt_opt= {\n    'criterion': 'squared_error',\n    'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression']}\n}\n\nxgb_opt = {\n    'n_estimators': tune.randint(50, 200),\n    'learning_rate': tune.loguniform(0.01, 0.5),   # Boosting learning rate\n    'max_depth': tune.randint(3, 10),              # Maximum depth of a tree\n    'min_child_weight': tune.randint(1, 10),        # Minimum sum of instance weight (hessian) needed in a child\n    }\n\n\nhypo_2 = {\n        'GBM': gbm_opt,\n        'RF': rf_opt,\n        'CAT': catb_opt,\n        'XT': xt_opt,\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor_new_hpo_2 = TabularPredictor(path='AutogluonModels/ag-hypo-2', \n                                     label=\"count\").fit(\n    train_data = df_train_added,\n    time_limit = 600,\n    presets= \"best_quality\",\n    hyperparameters = hypo_2,\n    hyperparameter_tune_kwargs = hyperpar_tune_2\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor_new_hpo_2.fit_summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor_new_hpo_2.leaderboard(silent=True).plot(kind=\"bar\", x=\"model\", y=\"score_val\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor_new_hpo_2.evaluate(df_train_added)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions\ncount_with_hpo_2 = predictor_new_hpo_2.predict(df_test_added)\n\n# Remember to set all negative values to zero\ncount_with_hpo_2[count_with_hpo_2 < 0] = 0\n\n# Same submitting predictions\nmy_datetime = test[\"datetime\"]\ncount_with_hpo_2\n\nsubmission_hpo_2 = pd.DataFrame(\n    list(zip(my_datetime, count_with_hpo_2)),\n    columns = [\"datetime\", \"count\"]\n)\n\nsubmission_hpo_2.to_csv(\"submission_hpo_2.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!kaggle competitions submit -c bike-sharing-demand -f submission_hpo_2.csv -m \"new features with hyperparameters: GBM, RF, CAT, XT\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Tuning 3","metadata":{}},{"cell_type":"code","source":"hyperpar_tune_3 = {\n    'searcher': 'random',\n    'max_tune_time': 4800,  # Maximum time to spend on hyperparameter tuning (in seconds)\n    \"scheduler\": \"local\",\n    \"num_trials\": 7\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define search space \ncatb_opt_new = {\n    'iterations': tune.randint(50, 200),\n    'learning_rate': tune.loguniform(0.01, 0.5),   # Learning rate\n    'depth': tune.randint(3, 10),                  # Depth of the trees\n    'l2_leaf_reg': tune.loguniform(1e-5, 100) \n    }\n\nxt_opt_new = {\n    'criterion': 'squared_error',\n    'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression']}\n}\n\nxgb_opt_new = {\n    'n_estimators': tune.randint(50, 200),\n    'learning_rate': tune.loguniform(0.01, 0.5),   # Boosting learning rate\n    'max_depth': tune.randint(3, 10),              # Maximum depth of a tree\n    'min_child_weight': tune.randint(1, 10),        # Minimum sum of instance weight (hessian) needed in a child\n    'subsample': tune.uniform(0.5, 1.0),           # Subsample ratio of the training instances\n    'colsample_bytree': tune.uniform(0.5, 1.0),    # Subsample ratio of columns when constructing each tree\n    'reg_alpha': tune.loguniform(1e-5, 100),       # L1 regularization term on weights\n    'reg_lambda': tune.loguniform(1e-5, 100)       # L2 regularization term on weights\n}\n\n\nhypo_3 = {\n        'CAT': catb_opt_new,\n        'XT': xt_opt_new,\n        'XGB': xgb_opt_new\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor_new_hpo_3 = TabularPredictor(path='AutogluonModels/ag-hypo-3', \n                                     label=\"count\").fit(\n    train_data = df_train_added,\n    time_limit = 4800,\n    presets= \"best_quality\",\n    hyperparameters = hypo_3,\n    hyperparameter_tune_kwargs = hyperpar_tune_3\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor_new_hpo_3.fit_summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor_new_hpo_3.leaderboard(silent=True).plot(kind=\"bar\", x=\"model\", y=\"score_val\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor_new_hpo_3.evaluate(df_train_added)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions\ncount_with_hpo_3 = predictor_new_hpo_3.predict(df_test_added)\n\n# Remember to set all negative values to zero\ncount_with_hpo_3[count_with_hpo_3 < 0] = 0\n\n# Same submitting predictions\nmy_datetime = test[\"datetime\"]\ncount_with_hpo_3\n\nsubmission_hpo_3 = pd.DataFrame(\n    list(zip(my_datetime, count_with_hpo_3)),\n    columns = [\"datetime\", \"count\"]\n)\n\nsubmission_hpo_3.to_csv(\"submission_hpo_3.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!kaggle competitions submit -c bike-sharing-demand -f submission_hpo_1.csv -m \"new features with hyperparameters: CAT, XT, XGB\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 7: Write a Report\n### Refer to the markdown file for the full report\n### Creating plots and table for report","metadata":{}},{"cell_type":"code","source":"# Taking the top model score from each training run and creating a line plot to show improvement\n# You can create these in the notebook and save them to PNG or use some other tool (e.g. google sheets, excel)\nfig = pd.DataFrame(\n    {\n        \"model\": [\"initial\", \"add_features\", \"hpo\"],\n        \"score\": [?, ?, ?]\n    }\n).plot(x=\"model\", y=\"score\", figsize=(8, 6)).get_figure()\nfig.savefig('model_train_score.png')","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAIN SCORE\n# Take the 4 model scores and creating a line plot to show improvement\nfig_train = pd.DataFrame(\n    {\n        \"train_eval\": [\"initial\", \"add_features\", \"hpo_1\", \"hpo_2\", \"hpo_3\"],\n        \"score\": [117.808, 18.456, 103.783, 16.157, 103.783]\n    }\n).plot(x=\"train_eval\", y=\"score\", figsize=(8, 6)).get_figure()\nfig_train.savefig('model_train_score.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TEST SCORE\n# Take the 4 kaggle scores and creating a line plot to show improvement\nfig = pd.DataFrame(\n    {\n        \"test_eval\": [\"initial\", \"add_features\", \"hpo_1\", \"hpo_2\", \"hpo_3\"],\n        \"score\": [1.32509, 0.48189, 1.19842, 0.46689, 1.19842]\n    }\n).plot(x=\"test_eval\", y=\"score\", figsize=(8, 6)).get_figure()\nfig.savefig('model_test_score.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter table","metadata":{}},{"cell_type":"code","source":"# The 3 hyperparameters we tuned with the kaggle score as the result\npd.DataFrame({\n    \"model_name\": [\"initial\",\n              \"add_features\",\n              \"hpo_1\",\n              \"hpo_2\",\n              \"hpo_3\"],\n    \"par1\": [\"original columns\",\n             \"extract year, month, day, hour from datetime\",\n             \"added features\",\n             \"added features\",\n             \"added features\"],\n    \"par2\": [\"time_limit=600\",\n            \"time_limit=600\",\n            \"hyperpar_tune={searcher:random,max_tune_time:3600,scheduler:local,num_trials:4}\",\n            \"hyperpar_tune=same like hpo1\",\n            \"hyperpar_tune=same like hpo2, change max_tune_time to 4800\",\n            ],\n    \"par3\": [\"presets=best_quality\",\n             \"presets=best_quality\",\n             \"CAT, NeuralNetFastAI\",\n             \"GBM, RF, CAT, FT\",\n             \"CAT, XT, XGB adding more search space in each model\"\n            ],\n    \"score\": [1.32509, 0.48189, 1.19842, 0.46689, 1.19842]\n})","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}